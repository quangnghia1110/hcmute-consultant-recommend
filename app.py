import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import json
import os
from pathlib import Path
from flask import Flask, request, jsonify, current_app
from flask_cors import CORS
from pyvi import ViTokenizer
import joblib
import re
import requests
import numpy as np
import google.generativeai as genai
import mysql.connector
from mysql.connector import Error
from config import (
    GOOGLE_API_KEY, GEMINI_MODEL, TEMPERATURE, TOP_P, TOP_K, MAX_OUTPUT_TOKENS,
    CURRENT_DIR, DATA_DIR, JSON_FILE, STOPWORDS_FILE, TFIDF_MATRIX_FILE, VECTORIZER_FILE,
    MYSQL_HOST, MYSQL_USER, MYSQL_PASSWORD, MYSQL_DATABASE, MYSQL_PORT, CHAT_URL
)

app = Flask(__name__)
CORS(app, resources={r"/*": {"origins": ["http://localhost:3000", "https://hcmute-consultant.vercel.app", "*"]}})

genai.configure(api_key=GOOGLE_API_KEY)

def create_mysql_connection():
    try:
        connection = mysql.connector.connect(
            host=MYSQL_HOST,
            port=MYSQL_PORT,
            user=MYSQL_USER,
            password=MYSQL_PASSWORD,
            database=MYSQL_DATABASE
        )
        if connection.is_connected():
            print("‚úÖ K·∫øt n·ªëi MySQL th√†nh c√¥ng")
            return connection
    except Error as e:
        print(f"L·ªói khi k·∫øt n·ªëi ƒë·∫øn MySQL: {e}")
    return None

def fetch_data_from_mysql():
    connection = create_mysql_connection()
    if not connection:
        print("‚ùå Kh√¥ng th·ªÉ k·∫øt n·ªëi ƒë·∫øn MySQL")
        return pd.DataFrame()

    try:
        print("üìä Ki·ªÉm tra t·ªïng s·ªë b·∫£n ghi")
        cursor = connection.cursor()
        cursor.execute("SELECT COUNT(*) FROM question")
        total_questions = cursor.fetchone()[0]
        cursor.execute("SELECT COUNT(*) FROM answer")
        total_answers = cursor.fetchone()[0]
        print(f"‚úÖ T·ªïng s·ªë: {total_questions} c√¢u h·ªèi, {total_answers} c√¢u tr·∫£ l·ªùi trong database")
        
        query_questions = """
        SELECT q.id, q.content, q.created_at, q.title, q.status_approval, q.role_ask_id, q.user_id
        FROM question q
        WHERE q.status_delete = 0
        """
        
        query_answers = """
        SELECT a.id, a.content, a.created_at, a.question_id, a.status_answer, a.status_approval,
               a.title, a.role_consultant_id, a.user_id
        FROM answer a
        """
        
        print("üìä Truy v·∫•n b·∫£ng question")
        questions_df = pd.read_sql(query_questions, connection)
        print(f"‚úÖ L·∫•y ƒë∆∞·ª£c {len(questions_df)} c√¢u h·ªèi")
        
        print("üìä Truy v·∫•n b·∫£ng answer")
        answers_df = pd.read_sql(query_answers, connection)
        print(f"‚úÖ L·∫•y ƒë∆∞·ª£c {len(answers_df)} c√¢u tr·∫£ l·ªùi")
        
        if questions_df.empty:
            print("‚ùå Kh√¥ng c√≥ d·ªØ li·ªáu trong b·∫£ng question th·ªèa m√£n ƒëi·ªÅu ki·ªán")
            return pd.DataFrame()
            
        if answers_df.empty:
            print("‚ùå Kh√¥ng c√≥ d·ªØ li·ªáu trong b·∫£ng answer th·ªèa m√£n ƒëi·ªÅu ki·ªán")
            return pd.DataFrame()
            
        print(f"üìä Th·ª±c hi·ªán merge hai b·∫£ng, question.id -> answer.question_id")
        print(f"üìä ID trong b·∫£ng question: {questions_df['id'].tolist()[:5]}")
        print(f"üìä question_id trong b·∫£ng answer: {answers_df['question_id'].tolist()[:5]}")
        
        merged_df = pd.merge(
            questions_df,
            answers_df,
            left_on='id',
            right_on='question_id',
            how='inner',
            suffixes=('_question', '_answer')
        )
        
        print(f"‚úÖ Sau khi merge c√≤n {len(merged_df)} d√≤ng")
        
        if merged_df.empty:
            print("‚ùå Kh√¥ng c√≥ d·ªØ li·ªáu sau khi merge")
            print("üìä Th·ª≠ th·ª±c hi·ªán left join ƒë·ªÉ xem v·∫•n ƒë·ªÅ")
            merged_df = pd.merge(
                questions_df,
                answers_df,
                left_on='id',
                right_on='question_id',
                how='left',
                suffixes=('_question', '_answer')
            )
            print(f"‚úÖ Sau khi left join c√≥ {len(merged_df)} d√≤ng")
            
            has_answer = merged_df['question_id'].notna().sum()
            print(f"üìä S·ªë c√¢u h·ªèi c√≥ c√¢u tr·∫£ l·ªùi: {has_answer}/{len(merged_df)}")
            
            if has_answer > 0:
                merged_df = merged_df[merged_df['question_id'].notna()]
                print(f"‚úÖ Gi·ªØ l·∫°i {len(merged_df)} c√¢u h·ªèi c√≥ c√¢u tr·∫£ l·ªùi")
        
        if merged_df.empty:
            print("‚ùå V·∫´n kh√¥ng c√≥ d·ªØ li·ªáu sau khi th·ª≠ c√°c ph∆∞∆°ng ph√°p join kh√°c nhau")
            return pd.DataFrame()
            
        mysql_df = pd.DataFrame({
            'question': merged_df['content_question'],
            'answer': merged_df['content_answer'],
            'question_id': merged_df['id_question'],
            'answer_id': merged_df['id_answer'],
            'source': 'mysql'
        })
        
        print(f"‚úÖ Tr·∫£ v·ªÅ DataFrame v·ªõi {len(mysql_df)} d√≤ng t·ª´ MySQL")
        return mysql_df
    
    except Error as e:
        print(f"‚ùå L·ªói khi truy v·∫•n MySQL: {e}")
        return pd.DataFrame()
    
    finally:
        if connection and connection.is_connected():
            connection.close()

def initialize_app(app):
    df, vectorizer, tfidf_matrix = prepare_data()
    app.config['df'] = df
    app.config['vectorizer'] = vectorizer
    app.config['tfidf_matrix'] = tfidf_matrix
    return True

def prepare_data():
    print("üìä ƒê·ªçc d·ªØ li·ªáu t·ª´ file JSON")
    json_df = load_json_data(JSON_FILE)
    if not json_df.empty:
        json_df['source'] = 'json'
        print(f"‚úÖ ƒê·ªçc ƒë∆∞·ª£c {len(json_df)} d√≤ng t·ª´ JSON")
    else:
        print("‚ö†Ô∏è Kh√¥ng ƒë·ªçc ƒë∆∞·ª£c d·ªØ li·ªáu t·ª´ JSON ho·∫∑c file JSON tr·ªëng")
        
    print("üìä ƒê·ªçc d·ªØ li·ªáu t·ª´ MySQL")
    mysql_df = fetch_data_from_mysql()
    if not mysql_df.empty:
        print(f"‚úÖ ƒê·ªçc ƒë∆∞·ª£c {len(mysql_df)} d√≤ng t·ª´ MySQL")
    else:
        print("‚ö†Ô∏è Kh√¥ng ƒë·ªçc ƒë∆∞·ª£c d·ªØ li·ªáu t·ª´ MySQL")
    
    # Ch·ªâ ph·ªëi h·ª£p d·ªØ li·ªáu n·∫øu c·∫£ hai ngu·ªìn ƒë·ªÅu c√≥ d·ªØ li·ªáu
    if not mysql_df.empty and not json_df.empty:
        print("üìä K·∫øt h·ª£p d·ªØ li·ªáu t·ª´ JSON v√† MySQL")
        df = pd.concat([json_df, mysql_df], ignore_index=True)
        print(f"‚úÖ T·ªïng s·ªë d√≤ng sau khi k·∫øt h·ª£p: {len(df)}")
    elif not mysql_df.empty:
        print("üìä Ch·ªâ s·ª≠ d·ª•ng d·ªØ li·ªáu t·ª´ MySQL")
        df = mysql_df
    else:
        print("üìä Ch·ªâ s·ª≠ d·ª•ng d·ªØ li·ªáu t·ª´ JSON")
        df = json_df
    
    # N·∫øu kh√¥ng c√≥ d·ªØ li·ªáu t·ª´ c·∫£ hai ngu·ªìn, t·∫°o DataFrame r·ªóng ƒë·ªÉ tr√°nh l·ªói
    if df.empty:
        print("‚ö†Ô∏è Kh√¥ng c√≥ d·ªØ li·ªáu t·ª´ c·∫£ MySQL v√† JSON, t·∫°o DataFrame r·ªóng")
        df = pd.DataFrame(columns=['question', 'answer'])
    
    print("üìä Chu·∫©n h√≥a v√† ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu")
    df['question'] = df['question'].astype(str).fillna('')
    df['answer'] = df['answer'].astype(str).fillna('')
    df = df.drop_duplicates(subset=['question'], keep='last').reset_index(drop=True)
    print(f"‚úÖ Sau khi lo·∫°i b·ªè tr√πng l·∫∑p c√≤n {len(df)} d√≤ng")
    
    print("üìä Tokenize d·ªØ li·ªáu ti·∫øng Vi·ªát")
    df['question_tokenized'] = df['question'].apply(tokenize_vietnamese)
    df['answer_tokenized'] = df['answer'].apply(tokenize_vietnamese)
    df['content'] = df['question_tokenized'] + ' ' + df['answer_tokenized']
    
    print("üìä T·∫£i stopwords")
    vietnamese_stopwords = load_stopwords()
    print(f"‚úÖ ƒê√£ t·∫£i {len(vietnamese_stopwords)} stopwords")

    print("üìä T·∫°o m√¥ h√¨nh TF-IDF")
    vectorizer, tfidf_matrix = create_tfidf_model(df, vietnamese_stopwords)
    print(f"‚úÖ ƒê√£ t·∫°o ma tr·∫≠n TF-IDF k√≠ch th∆∞·ªõc {tfidf_matrix.shape}")
    
    return df, vectorizer, tfidf_matrix

def load_json_data(json_file):
    try:
        json_path = get_data_path(json_file)
        if not json_path.exists():
            return pd.DataFrame(columns=['question', 'answer'])
        with open(json_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        df = pd.DataFrame(data)
        if not all(col in df.columns for col in ['question', 'answer']):
            return pd.DataFrame(columns=['question', 'answer'])
        return df
    except Exception as e:
        print(f"L·ªói khi ƒë·ªçc file JSON: {str(e)}")
        return pd.DataFrame(columns=['question', 'answer'])
    
def get_data_path(filename):
    data_path = DATA_DIR / filename
    if data_path.exists():
        return data_path
    current_path = CURRENT_DIR / filename
    if current_path.exists():
        return current_path
    return Path(filename)

def load_stopwords():
    try:
        stopwords_path = get_data_path(STOPWORDS_FILE)
        if not stopwords_path.exists():
            return []
        with open(stopwords_path, 'r', encoding='utf-8') as f:
            stopwords = [line.strip() for line in f if line.strip()]
        return stopwords
    except Exception as e:
        print(f"L·ªói khi ƒë·ªçc stopwords: {str(e)}")
        return []

def tokenize_vietnamese(text):
    if not isinstance(text, str) or not text.strip():
        return ""
    text = text.lower()
    text = re.sub(r'[^\w\s]', '', text)
    return " ".join(ViTokenizer.tokenize(text).split())

def create_tfidf_model(df, stopwords):
    vectorizer = TfidfVectorizer(
        min_df=2,
        max_features=10000,
        strip_accents='unicode',
        analyzer='word',
        token_pattern=r'\w{1,}',
        ngram_range=(1, 2),
        stop_words=stopwords
    )
    if len(df) > 0:
        tfidf_matrix = vectorizer.fit_transform(df['content'])
    else:
        tfidf_matrix = vectorizer.fit_transform(["fallback content"])
    try:
        tfidf_path = get_data_path(TFIDF_MATRIX_FILE)
        vectorizer_path = get_data_path(VECTORIZER_FILE)
        joblib.dump(tfidf_matrix, DATA_DIR / tfidf_path)
        joblib.dump(vectorizer, DATA_DIR / vectorizer_path)
    except Exception as e:
        print(f"L·ªói khi l∆∞u m√¥ h√¨nh TF-IDF: {str(e)}")
    return vectorizer, tfidf_matrix

def recommend_similar_questions(query, top_n=5):
    try:
        vectorizer = app.config['vectorizer']
        tfidf_matrix = app.config['tfidf_matrix']
        query_tokenized = tokenize_vietnamese(query)
        query_tfidf = vectorizer.transform([query_tokenized])
        sim_scores = cosine_similarity(query_tfidf, tfidf_matrix)[0]
        sim_scores_with_indices = []
        for idx, score in enumerate(sim_scores):
            if score > 0.1:
                sim_scores_with_indices.append((idx, score))
        sim_scores_with_indices = sorted(sim_scores_with_indices, 
                                         key=lambda x: x[1], 
                                         reverse=True)
        top_results = sim_scores_with_indices[:top_n]
        question_indices = [i[0] for i in top_results]
        question_scores = [i[1] for i in top_results]
        return question_indices, question_scores
    except Exception as e:
        print(f"L·ªói khi t·∫°o g·ª£i √Ω: {str(e)}")
        return [], []

@app.route('/recommend', methods=['GET'])
def recommend():
    try:
        query = request.args.get('text', '').strip()
        if not query:
            return jsonify({
                'status': 'error',
                'message': 'Tham s·ªë truy v·∫•n "text" l√† b·∫Øt bu·ªôc v√† kh√¥ng ƒë∆∞·ª£c r·ªóng'
            }), 400
        recommended_indices, similarity_scores = recommend_similar_questions(query, 5)
        if not recommended_indices or not similarity_scores:
            return jsonify({
                'status': 'success',
                'message': f'Kh√¥ng t√¨m th·∫•y g·ª£i √Ω ph√π h·ª£p cho truy v·∫•n "{query}"',
                'data': []
            })
        df = current_app.config['df']
        recommendations = []
        for idx, score in zip(recommended_indices, similarity_scores):
            if idx < len(df) and score > 0.3:
                result = {
                    'question': df.iloc[idx]['question'],
                    'answer': df.iloc[idx]['answer'],
                    'similarity_score': float(score)
                }
                
                if 'source' in df.columns:
                    result['source'] = df.iloc[idx]['source']
                    
                if 'question_id' in df.columns and not pd.isna(df.iloc[idx].get('question_id')):
                    result['question_id'] = int(df.iloc[idx]['question_id'])
                    
                if 'answer_id' in df.columns and not pd.isna(df.iloc[idx].get('answer_id')):
                    result['answer_id'] = int(df.iloc[idx]['answer_id'])
                
                recommendations.append(result)
                
        if not recommendations:
            return jsonify({
                'status': 'success',
                'message': f'Kh√¥ng t√¨m th·∫•y g·ª£i √Ω ph√π h·ª£p cho truy v·∫•n "{query}"',
                'data': []
            })
        return jsonify({
            'status': 'success',
            'message': f'ƒê√£ g·ª£i √Ω {len(recommendations)} m·ª•c cho truy v·∫•n "{query}"',
            'data': recommendations
        })
    except Exception as e:
        print(f"L·ªói trong endpoint /recommend: {str(e)}")
        return jsonify({
            'status': 'error',
            'message': f'L·ªói m√°y ch·ªß n·ªôi b·ªô: {str(e)}'
        }), 500

@app.route('/recommend-answers', methods=['GET'])
def get_recommend_answers():
    try:
        query = request.args.get('text', '').strip()
        if not query:
            return jsonify({
                'status': 'error',
                'message': 'Tham s·ªë truy v·∫•n "text" l√† b·∫Øt bu·ªôc v√† kh√¥ng ƒë∆∞·ª£c r·ªóng'
            }), 400
        chat_url = f"{CHAT_URL}/chat?text={query}"
        response = requests.get(chat_url)
        if response.status_code != 200:
            return jsonify({
                'status': 'error',
                'message': f'Kh√¥ng th·ªÉ l·∫•y d·ªØ li·ªáu t·ª´ API chat: {response.status_code}'
            }), 500
        chat_data = response.json()
        question = chat_data['data']['question']
        answer = chat_data['data']['answer']
        alternative_answers = generate_alternative_answers(question, answer)
        if len(alternative_answers) > 5:
            alternative_answers = alternative_answers[:5]
        result_answers = []
        for a in alternative_answers:
            result_answers.append({
                "answer": a
            })
        return jsonify({
            'status': 'success',
            'message': 'ƒê√£ t·∫°o 5 c√¢u tr·∫£ l·ªùi thay th·∫ø',
            'question': question,
            'answer': answer,
            'alternative_answers': result_answers
        })
    except Exception as e:
        print(f"L·ªói trong endpoint /recommend-answers: {str(e)}")
        return jsonify({
            'status': 'error',
            'message': f'L·ªói m√°y ch·ªß n·ªôi b·ªô: {str(e)}'
        }), 500

def generate_alternative_answers(question, answer):
    try:
        prompt = f"""
            D·ª±a v√†o c√¢u h·ªèi v√† c√¢u tr·∫£ l·ªùi g·ªëc d∆∞·ªõi ƒë√¢y, h√£y t·∫°o ch√≠nh x√°c 5 c√¢u tr·∫£ l·ªùi thay th·∫ø KH√ÅC BI·ªÜT HO√ÄN TO√ÄN v·ªÅ c√°ch tr√¨nh b√†y.
            M·ªñI c√¢u tr·∫£ l·ªùi PH·∫¢I c√≥:
            - ƒê·ªô d√†i kh√°c nhau (ng·∫Øn, trung b√¨nh, d√†i)
            - C√°ch ti·∫øp c·∫≠n kh√°c nhau (tr·ª±c ti·∫øp, chi ti·∫øt, v√≠ d·ª• th·ª±c t·∫ø, d∆∞·ªõi d·∫°ng h∆∞·ªõng d·∫´n)
            - Gi·ªçng ƒëi·ªáu kh√°c nhau (trang tr·ªçng, th√¢n thi·ªán, chuy√™n nghi·ªáp, ƒë∆°n gi·∫£n)
            
            C√ÇU H·ªéI: {question}
            C√ÇU TR·∫¢ L·ªúI G·ªêC: {answer}
            
            CH·ªà TR·∫¢ V·ªÄ 5 C√ÇU TR·∫¢ L·ªúI THAY TH·∫æ, M·ªñI C√ÇU TR√äN 1 ƒêO·∫†N VƒÇN, KH√îNG ƒê√ÅNH S·ªê, KH√îNG TH√äM B·∫§T K·ª≤ GI·∫¢I TH√çCH N√ÄO KH√ÅC.
        """
        model = genai.GenerativeModel(GEMINI_MODEL)
        response = model.generate_content(
            prompt,
            generation_config=genai.GenerationConfig(
                temperature=TEMPERATURE,
                top_p=TOP_P,
                top_k=TOP_K,
                max_output_tokens=MAX_OUTPUT_TOKENS,
            )
        )
        if hasattr(response, 'text'):
            raw_answers = []
            current_answer = ""
            for line in response.text.strip().split('\n'):
                if line.strip():
                    if not current_answer:
                        current_answer = line.strip()
                    else:
                        current_answer += " " + line.strip()
                else:
                    if current_answer:
                        raw_answers.append(current_answer)
                        current_answer = ""
            if current_answer:
                raw_answers.append(current_answer)
            
            return raw_answers
        else:
            print("Gemini API kh√¥ng tr·∫£ v·ªÅ k·∫øt qu·∫£ d·∫°ng text")
            return []
    except Exception as e:
        print(f"L·ªói khi g·ªçi Gemini API: {str(e)}")
        return []

@app.route('/refresh-data', methods=['POST'])
def refresh_data():
    try:
        if initialize_app(app):
            return jsonify({
                'status': 'success',
                'message': 'D·ªØ li·ªáu v√† model ƒë√£ ƒë∆∞·ª£c l√†m m·ªõi th√†nh c√¥ng'
            })
        else:
            return jsonify({
                'status': 'error',
                'message': 'C√≥ l·ªói x·∫£y ra khi l√†m m·ªõi d·ªØ li·ªáu'
            }), 500
    except Exception as e:
        return jsonify({
            'status': 'error',
            'message': f'L·ªói m√°y ch·ªß n·ªôi b·ªô: {str(e)}'
        }), 500

if __name__ == "__main__":
    initialize_app(app)
    print("‚úÖ D·ªØ li·ªáu ƒë√£ kh·ªüi t·∫°o")
    port = int(os.environ.get('PORT', 4000))
    app.run(host='0.0.0.0', port=port, debug=False) 